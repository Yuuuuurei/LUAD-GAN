"""
Quality Metrics Module for GAN-LUAD Clustering Project
Phase 6: Synthetic Data Quality Validation

This module provides comprehensive quality metrics for evaluating
synthetic data generated by GANs, including:
- Statistical similarity tests
- Distribution comparisons
- Correlation analysis
- Biological plausibility checks

Author: GAN-LUAD Team
Date: 2025
"""

import numpy as np
import torch
from typing import Dict, Tuple, List, Optional, Union
from scipy import stats
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import logging
import json
from pathlib import Path

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class QualityMetrics:
    """
    Comprehensive quality metrics for synthetic data validation.
    """
    
    def __init__(self, real_data: np.ndarray, synthetic_data: np.ndarray):
        """
        Initialize QualityMetrics.
        
        Args:
            real_data: Real data samples (n_real, n_features)
            synthetic_data: Synthetic data samples (n_synthetic, n_features)
        """
        self.real_data = self._to_numpy(real_data)
        self.synthetic_data = self._to_numpy(synthetic_data)
        
        logger.info(f"Real data shape: {self.real_data.shape}")
        logger.info(f"Synthetic data shape: {self.synthetic_data.shape}")
    
    @staticmethod
    def _to_numpy(data: Union[np.ndarray, torch.Tensor]) -> np.ndarray:
        """Convert data to numpy array if needed."""
        if isinstance(data, torch.Tensor):
            return data.cpu().numpy()
        return data
    
    def compute_all_metrics(self) -> Dict:
        """
        Compute all quality metrics.
        
        Returns:
            Dictionary containing all metrics and overall quality score
        """
        logger.info("Computing all quality metrics...")
        
        metrics = {
            'statistical': self.compute_statistical_metrics(),
            'distribution': self.compute_distribution_metrics(),
            'correlation': self.compute_correlation_metrics(),
            'dimensionality': self.compute_dimensionality_metrics()
        }
        
        # Compute overall quality score
        metrics['quality_score'] = self.compute_quality_score(metrics)
        
        logger.info(f"Quality score: {metrics['quality_score']['score']:.2f}/4.0")
        logger.info(f"Quality level: {metrics['quality_score']['level']}")
        
        return metrics
    
    def compute_statistical_metrics(self) -> Dict:
        """
        Compute basic statistical metrics comparing real and synthetic data.
        
        Returns:
            Dictionary with mean_diff, variance_ratio, std_diff
        """
        logger.info("Computing statistical metrics...")
        
        # Feature-wise statistics
        real_mean = self.real_data.mean(axis=0)
        synthetic_mean = self.synthetic_data.mean(axis=0)
        
        real_var = self.real_data.var(axis=0)
        synthetic_var = self.synthetic_data.var(axis=0)
        
        real_std = self.real_data.std(axis=0)
        synthetic_std = self.synthetic_data.std(axis=0)
        
        # Compute differences
        mean_diff = np.abs(real_mean - synthetic_mean).mean()
        std_diff = np.abs(real_std - synthetic_std).mean()
        
        # Variance ratio (should be close to 1.0)
        variance_ratio = (synthetic_var / (real_var + 1e-10)).mean()
        
        # Min/Max ranges
        real_min, real_max = self.real_data.min(), self.real_data.max()
        syn_min, syn_max = self.synthetic_data.min(), self.synthetic_data.max()
        
        metrics = {
            'mean_difference': float(mean_diff),
            'std_difference': float(std_diff),
            'variance_ratio': float(variance_ratio),
            'real_range': (float(real_min), float(real_max)),
            'synthetic_range': (float(syn_min), float(syn_max)),
            'range_overlap': self._compute_range_overlap(
                (real_min, real_max), (syn_min, syn_max)
            )
        }
        
        logger.info(f"  Mean difference: {mean_diff:.4f}")
        logger.info(f"  Variance ratio: {variance_ratio:.4f}")
        logger.info(f"  Std difference: {std_diff:.4f}")
        
        return metrics
    
    def compute_distribution_metrics(self) -> Dict:
        """
        Compute distribution similarity metrics.
        
        Uses Kolmogorov-Smirnov test and Wasserstein distance.
        
        Returns:
            Dictionary with KS statistics and Wasserstein distances
        """
        logger.info("Computing distribution metrics...")
        
        n_features = self.real_data.shape[1]
        
        # Sample features for testing (testing all can be slow)
        sample_size = min(100, n_features)
        feature_indices = np.random.choice(n_features, sample_size, replace=False)
        
        ks_statistics = []
        ks_pvalues = []
        wasserstein_distances = []
        
        for idx in feature_indices:
            # Kolmogorov-Smirnov test
            ks_stat, ks_pval = stats.ks_2samp(
                self.real_data[:, idx],
                self.synthetic_data[:, idx]
            )
            ks_statistics.append(ks_stat)
            ks_pvalues.append(ks_pval)
            
            # Wasserstein distance (Earth Mover's Distance)
            wd = stats.wasserstein_distance(
                self.real_data[:, idx],
                self.synthetic_data[:, idx]
            )
            wasserstein_distances.append(wd)
        
        metrics = {
            'ks_statistic_mean': float(np.mean(ks_statistics)),
            'ks_statistic_std': float(np.std(ks_statistics)),
            'ks_pvalue_mean': float(np.mean(ks_pvalues)),
            'wasserstein_distance_mean': float(np.mean(wasserstein_distances)),
            'wasserstein_distance_std': float(np.std(wasserstein_distances)),
            'n_features_tested': sample_size
        }
        
        logger.info(f"  KS statistic (mean): {metrics['ks_statistic_mean']:.4f}")
        logger.info(f"  Wasserstein distance (mean): {metrics['wasserstein_distance_mean']:.4f}")
        
        return metrics
    
    def compute_correlation_metrics(self) -> Dict:
        """
        Compute correlation structure similarity.
        
        Returns:
            Dictionary with correlation differences
        """
        logger.info("Computing correlation metrics...")
        
        # Sample features for correlation (computing full corr can be slow)
        n_features = self.real_data.shape[1]
        sample_size = min(500, n_features)
        feature_indices = np.random.choice(n_features, sample_size, replace=False)
        
        real_subset = self.real_data[:, feature_indices]
        synthetic_subset = self.synthetic_data[:, feature_indices]
        
        # Compute correlation matrices
        real_corr = np.corrcoef(real_subset.T)
        synthetic_corr = np.corrcoef(synthetic_subset.T)
        
        # Handle NaN values
        real_corr = np.nan_to_num(real_corr, nan=0.0)
        synthetic_corr = np.nan_to_num(synthetic_corr, nan=0.0)
        
        # Correlation difference (Frobenius norm)
        corr_diff = np.linalg.norm(real_corr - synthetic_corr, 'fro')
        corr_diff_normalized = corr_diff / (sample_size ** 2)
        
        # Mean absolute correlation difference
        mean_abs_corr_diff = np.abs(real_corr - synthetic_corr).mean()
        
        metrics = {
            'correlation_difference': float(corr_diff_normalized),
            'mean_abs_correlation_diff': float(mean_abs_corr_diff),
            'n_features_tested': sample_size
        }
        
        logger.info(f"  Correlation difference: {corr_diff_normalized:.4f}")
        logger.info(f"  Mean abs correlation diff: {mean_abs_corr_diff:.4f}")
        
        return metrics
    
    def compute_dimensionality_metrics(self) -> Dict:
        """
        Compute metrics based on dimensionality reduction.
        
        Uses PCA to check if synthetic data covers the same space as real data.
        
        Returns:
            Dictionary with PCA-based metrics
        """
        logger.info("Computing dimensionality metrics...")
        
        # Fit PCA on real data
        pca = PCA(n_components=50)
        pca.fit(self.real_data)
        
        # Transform both datasets
        real_pca = pca.transform(self.real_data)
        synthetic_pca = pca.transform(self.synthetic_data)
        
        # Compare variance explained
        real_var_explained = np.var(real_pca, axis=0)
        synthetic_var_explained = np.var(synthetic_pca, axis=0)
        
        variance_ratio = synthetic_var_explained / (real_var_explained + 1e-10)
        
        # Compare PC space coverage (mean and std of each PC)
        pc_mean_diff = np.abs(real_pca.mean(axis=0) - synthetic_pca.mean(axis=0)).mean()
        pc_std_diff = np.abs(real_pca.std(axis=0) - synthetic_pca.std(axis=0)).mean()
        
        metrics = {
            'pca_variance_ratio_mean': float(variance_ratio.mean()),
            'pca_variance_ratio_std': float(variance_ratio.std()),
            'pc_mean_difference': float(pc_mean_diff),
            'pc_std_difference': float(pc_std_diff),
            'n_components': 50
        }
        
        logger.info(f"  PCA variance ratio: {metrics['pca_variance_ratio_mean']:.4f}")
        logger.info(f"  PC mean difference: {pc_mean_diff:.4f}")
        
        return metrics
    
    @staticmethod
    def _compute_range_overlap(range1: Tuple[float, float], range2: Tuple[float, float]) -> float:
        """Compute overlap between two ranges (0 to 1)."""
        min1, max1 = range1
        min2, max2 = range2
        
        overlap_min = max(min1, min2)
        overlap_max = min(max1, max2)
        
        if overlap_max <= overlap_min:
            return 0.0
        
        overlap_length = overlap_max - overlap_min
        total_range = max(max1, max2) - min(min1, min2)
        
        return overlap_length / total_range if total_range > 0 else 0.0
    
    def compute_quality_score(self, metrics: Optional[Dict] = None) -> Dict:
        """
        Compute overall quality score (0-4 scale).
        
        Criteria:
        - Mean difference < 0.1 → 1 point
        - Variance ratio 0.8-1.2 → 1 point
        - KS statistic < 0.15 → 1 point
        - Correlation diff < 0.2 → 1 point
        
        Returns:
            Dictionary with score and interpretation
        """
        if metrics is None:
            metrics = self.compute_all_metrics()
        
        score = 0
        details = {}
        
        # Test 1: Mean difference
        mean_diff = metrics['statistical']['mean_difference']
        if mean_diff < 0.1:
            score += 1
            details['mean_test'] = 'PASS'
        else:
            details['mean_test'] = f'FAIL (diff={mean_diff:.4f})'
        
        # Test 2: Variance ratio
        var_ratio = metrics['statistical']['variance_ratio']
        if 0.8 <= var_ratio <= 1.2:
            score += 1
            details['variance_test'] = 'PASS'
        else:
            details['variance_test'] = f'FAIL (ratio={var_ratio:.4f})'
        
        # Test 3: KS statistic
        ks_stat = metrics['distribution']['ks_statistic_mean']
        if ks_stat < 0.15:
            score += 1
            details['distribution_test'] = 'PASS'
        else:
            details['distribution_test'] = f'FAIL (KS={ks_stat:.4f})'
        
        # Test 4: Correlation difference
        corr_diff = metrics['correlation']['mean_abs_correlation_diff']
        if corr_diff < 0.2:
            score += 1
            details['correlation_test'] = 'PASS'
        else:
            details['correlation_test'] = f'FAIL (diff={corr_diff:.4f})'
        
        # Interpret score
        if score == 4:
            level = "Excellent"
            recommendation = "High quality - ready for clustering"
        elif score == 3:
            level = "Good"
            recommendation = "Good quality - usable for clustering"
        elif score == 2:
            level = "Acceptable"
            recommendation = "Acceptable - may need hyperparameter tuning"
        else:
            level = "Poor"
            recommendation = "Poor quality - retrain GAN with different settings"
        
        return {
            'score': score,
            'max_score': 4,
            'level': level,
            'recommendation': recommendation,
            'details': details
        }
    
    def generate_report(self, save_path: Optional[Union[str, Path]] = None) -> Dict:
        """
        Generate comprehensive quality report.
        
        Args:
            save_path: Optional path to save report as JSON
            
        Returns:
            Complete quality report dictionary
        """
        logger.info("Generating quality report...")
        
        # Compute all metrics
        metrics = self.compute_all_metrics()
        
        # Add data info
        report = {
            'data_info': {
                'n_real_samples': self.real_data.shape[0],
                'n_synthetic_samples': self.synthetic_data.shape[0],
                'n_features': self.real_data.shape[1],
                'synthetic_to_real_ratio': self.synthetic_data.shape[0] / self.real_data.shape[0]
            },
            'metrics': metrics,
            'summary': self._generate_summary(metrics)
        }
        
        # Save if path provided
        if save_path is not None:
            save_path = Path(save_path)
            save_path.parent.mkdir(parents=True, exist_ok=True)
            
            with open(save_path, 'w') as f:
                json.dump(report, f, indent=2)
            
            logger.info(f"Report saved to: {save_path}")
        
        return report
    
    def _generate_summary(self, metrics: Dict) -> str:
        """Generate human-readable summary."""
        quality = metrics['quality_score']
        
        summary = f"""
Quality Assessment Summary
{'='*80}

Overall Quality Score: {quality['score']}/4 - {quality['level']}
Recommendation: {quality['recommendation']}

Test Results:
  - Mean Difference Test: {quality['details']['mean_test']}
  - Variance Ratio Test: {quality['details']['variance_test']}
  - Distribution Test: {quality['details']['distribution_test']}
  - Correlation Test: {quality['details']['correlation_test']}

Key Metrics:
  - Mean Difference: {metrics['statistical']['mean_difference']:.4f}
  - Variance Ratio: {metrics['statistical']['variance_ratio']:.4f}
  - KS Statistic: {metrics['distribution']['ks_statistic_mean']:.4f}
  - Correlation Diff: {metrics['correlation']['mean_abs_correlation_diff']:.4f}

{'='*80}
        """
        return summary.strip()
    
    def print_summary(self):
        """Print quality summary to console."""
        metrics = self.compute_all_metrics()
        print(self._generate_summary(metrics))


def compare_multiple_augmentations(
    real_data: np.ndarray,
    augmentations: Dict[str, np.ndarray]
) -> Dict[str, Dict]:
    """
    Compare quality metrics across multiple augmentations.
    
    Args:
        real_data: Real data samples
        augmentations: Dictionary mapping names to synthetic datasets
        
    Returns:
        Dictionary mapping names to quality metrics
    """
    logger.info(f"Comparing {len(augmentations)} augmentations...")
    
    results = {}
    
    for name, synthetic_data in augmentations.items():
        logger.info(f"\nEvaluating: {name}")
        logger.info("="*80)
        
        qm = QualityMetrics(real_data, synthetic_data)
        metrics = qm.compute_all_metrics()
        
        results[name] = metrics
    
    # Find best augmentation
    best_name = max(results.keys(), key=lambda k: results[k]['quality_score']['score'])
    best_score = results[best_name]['quality_score']['score']
    
    logger.info(f"\nBest augmentation: {best_name} (score: {best_score}/4)")
    
    return results


# Example usage
if __name__ == "__main__":
    print("Quality Metrics Module - Phase 6")
    print("="*80)
    
    # Example with dummy data
    print("\nExample usage:")
    print("""
    from backend.validation.quality_metrics import QualityMetrics
    
    # Load data
    real_data = np.load('data/processed/luad_processed.npz')['data']
    synthetic_data = np.load('data/synthetic/gan_generated_samples.npz')['data']
    
    # Initialize quality metrics
    qm = QualityMetrics(real_data, synthetic_data)
    
    # Compute all metrics
    metrics = qm.compute_all_metrics()
    
    # Print summary
    qm.print_summary()
    
    # Generate and save report
    report = qm.generate_report('results/gan_validation/quality_report.json')
    
    # Quality score
    print(f"Quality Score: {metrics['quality_score']['score']}/4")
    print(f"Level: {metrics['quality_score']['level']}")
    """)
    
    print("\n" + "="*80)
    print("Module ready for use!")